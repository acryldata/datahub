"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[75963],{15680:(e,a,t)=>{t.d(a,{xA:()=>p,yg:()=>c});var n=t(96540);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function o(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?i(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=n.createContext({}),d=function(e){var a=n.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):o(o({},a),e)),t},p=function(e){var a=d(e.components);return n.createElement(l.Provider,{value:a},e.children)},u="mdxType",g={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},m=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=d(t),m=r,c=u["".concat(l,".").concat(m)]||u[m]||g[m]||i;return t?n.createElement(c,o(o({ref:a},p),{},{components:t})):n.createElement(c,o({ref:a},p))}));function c(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var i=t.length,o=new Array(i);o[0]=m;var s={};for(var l in a)hasOwnProperty.call(a,l)&&(s[l]=a[l]);s.originalType=e,s[u]="string"==typeof e?e:r,o[1]=s;for(var d=2;d<i;d++)o[d]=t[d];return n.createElement.apply(null,o)}return n.createElement.apply(null,t)}m.displayName="MDXCreateElement"},77696:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>p,contentTitle:()=>l,default:()=>c,frontMatter:()=>s,metadata:()=>d,toc:()=>u});t(96540);var n=t(15680);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){return a=null!=a?a:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):function(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))})),e}function o(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}const s={sidebar_position:2,title:"DataJob",slug:"/generated/metamodel/entities/datajob-datahub",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/generated/metamodel/entities/dataJob-datahub.md"},l="DataJob",d={unversionedId:"docs/generated/metamodel/entities/dataJob-datahub",id:"docs/generated/metamodel/entities/dataJob-datahub",title:"DataJob",description:"Data jobs represent individual units of data processing work within a data pipeline or workflow. They are the tasks, steps, or operations that transform, move, or process data as part of a larger data flow. Examples include Airflow tasks, dbt models, Spark jobs, Databricks notebooks, and similar processing units in orchestration systems.",source:"@site/genDocs/docs/generated/metamodel/entities/dataJob-datahub.md",sourceDirName:"docs/generated/metamodel/entities",slug:"/generated/metamodel/entities/datajob-datahub",permalink:"/docs/generated/metamodel/entities/datajob-datahub",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/generated/metamodel/entities/dataJob-datahub.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,title:"DataJob",slug:"/generated/metamodel/entities/datajob-datahub",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/generated/metamodel/entities/dataJob-datahub.md"},sidebar:"overviewSidebar",previous:{title:"Dataset",permalink:"/docs/generated/metamodel/entities/dataset"},next:{title:"DataJob",permalink:"/docs/generated/metamodel/entities/datajob"}},p={},u=[{value:"Identity",id:"identity",level:2},{value:"Examples",id:"examples",level:3},{value:"Important Capabilities",id:"important-capabilities",level:2},{value:"Job Information (dataJobInfo)",id:"job-information-datajobinfo",level:3},{value:"Input/Output Lineage (dataJobInputOutput)",id:"inputoutput-lineage-datajobinputoutput",level:3},{value:"Editable Properties (editableDataJobProperties)",id:"editable-properties-editabledatajobproperties",level:3},{value:"Ownership",id:"ownership",level:3},{value:"Tags and Glossary Terms",id:"tags-and-glossary-terms",level:3},{value:"Domains and Applications",id:"domains-and-applications",level:3},{value:"Structured Properties and Forms",id:"structured-properties-and-forms",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"Creating a Data Job",id:"creating-a-data-job",level:3},{value:"Adding Tags, Terms, and Ownership",id:"adding-tags-terms-and-ownership",level:3},{value:"Updating Job Properties",id:"updating-job-properties",level:3},{value:"Querying Data Job Information",id:"querying-data-job-information",level:3},{value:"Adding Lineage to Data Jobs",id:"adding-lineage-to-data-jobs",level:3},{value:"Integration Points",id:"integration-points",level:2},{value:"Relationship with DataFlow",id:"relationship-with-dataflow",level:3},{value:"Relationship with Datasets",id:"relationship-with-datasets",level:3},{value:"Relationship with DataProcessInstance",id:"relationship-with-dataprocessinstance",level:3},{value:"GraphQL Resolvers",id:"graphql-resolvers",level:3},{value:"Ingestion Sources",id:"ingestion-sources",level:3},{value:"Notable Exceptions",id:"notable-exceptions",level:2},{value:"DataHub Ingestion Jobs",id:"datahub-ingestion-jobs",level:3},{value:"Job Status Deprecation",id:"job-status-deprecation",level:3},{value:"Subtype Usage",id:"subtype-usage",level:3},{value:"Technical Reference",id:"technical-reference",level:2}],g={toc:u},m="wrapper";function c(e){var{components:a}=e,t=o(e,["components"]);return(0,n.yg)(m,i(function(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{},n=Object.keys(t);"function"==typeof Object.getOwnPropertySymbols&&(n=n.concat(Object.getOwnPropertySymbols(t).filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable})))),n.forEach((function(a){r(e,a,t[a])}))}return e}({},g,t),{components:a,mdxType:"MDXLayout"}),(0,n.yg)("h1",{id:"datajob"},"DataJob"),(0,n.yg)("p",null,"Data jobs represent individual units of data processing work within a data pipeline or workflow. They are the tasks, steps, or operations that transform, move, or process data as part of a larger data flow. Examples include Airflow tasks, dbt models, Spark jobs, Databricks notebooks, and similar processing units in orchestration systems."),(0,n.yg)("h2",{id:"identity"},"Identity"),(0,n.yg)("p",null,"Data jobs are identified by two pieces of information:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"The data flow (pipeline/workflow) that they belong to: this is represented as a URN pointing to the parent ",(0,n.yg)("inlineCode",{parentName:"li"},"dataFlow")," entity. The data flow defines the orchestrator (e.g., ",(0,n.yg)("inlineCode",{parentName:"li"},"airflow"),", ",(0,n.yg)("inlineCode",{parentName:"li"},"spark"),", ",(0,n.yg)("inlineCode",{parentName:"li"},"dbt"),"), the flow ID (e.g., the DAG name or pipeline name), and the cluster where it runs."),(0,n.yg)("li",{parentName:"ul"},"The unique job identifier within that flow: this is a string that uniquely identifies the task within its parent flow (e.g., task name, step name, model name).")),(0,n.yg)("p",null,"The URN structure for a data job is: ",(0,n.yg)("inlineCode",{parentName:"p"},"urn:li:dataJob:(urn:li:dataFlow:(<orchestrator>,<flow_id>,<cluster>),<job_id>)")),(0,n.yg)("h3",{id:"examples"},"Examples"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Airflow task:")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"urn:li:dataJob:(urn:li:dataFlow:(airflow,daily_etl_dag,prod),transform_customer_data)\n")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"dbt model:")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"urn:li:dataJob:(urn:li:dataFlow:(dbt,analytics_project,prod),staging.stg_customers)\n")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Spark job:")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"urn:li:dataJob:(urn:li:dataFlow:(spark,data_processing_pipeline,PROD),aggregate_sales_task)\n")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Databricks notebook:")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"urn:li:dataJob:(urn:li:dataFlow:(databricks,etl_workflow,production),process_events_notebook)\n")),(0,n.yg)("h2",{id:"important-capabilities"},"Important Capabilities"),(0,n.yg)("h3",{id:"job-information-datajobinfo"},"Job Information (dataJobInfo)"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"dataJobInfo")," aspect captures the core properties of a data job:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Name"),": Human-readable name of the job (searchable with autocomplete)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Description"),": Detailed description of what the job does"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Type"),": The type of job (e.g., SQL, Python, Spark, etc.)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Flow URN"),": Reference to the parent data flow"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Created/Modified timestamps"),": When the job was created or last modified in the source system"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Environment"),": The fabric/environment where the job runs (PROD, DEV, QA, etc.)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Custom properties"),": Additional key-value properties specific to the source system"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"External references"),": Links to external documentation or definitions (e.g., GitHub links)")),(0,n.yg)("h3",{id:"inputoutput-lineage-datajobinputoutput"},"Input/Output Lineage (dataJobInputOutput)"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"dataJobInputOutput")," aspect defines the data lineage relationships for the job:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Input datasets"),": Datasets consumed by the job during processing (via ",(0,n.yg)("inlineCode",{parentName:"li"},"inputDatasetEdges"),")"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Output datasets"),": Datasets produced by the job (via ",(0,n.yg)("inlineCode",{parentName:"li"},"outputDatasetEdges"),")"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Input data jobs"),": Other data jobs that this job depends on (via ",(0,n.yg)("inlineCode",{parentName:"li"},"inputDatajobEdges"),")"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Input dataset fields"),": Specific schema fields consumed from input datasets"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Output dataset fields"),": Specific schema fields produced in output datasets"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Fine-grained lineage"),": Column-level lineage mappings showing which upstream fields contribute to downstream fields")),(0,n.yg)("p",null,"This aspect establishes the critical relationships that enable DataHub to build and visualize data lineage graphs across your entire data ecosystem."),(0,n.yg)("h3",{id:"editable-properties-editabledatajobproperties"},"Editable Properties (editableDataJobProperties)"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"editableDataJobProperties")," aspect stores documentation edits made through the DataHub UI:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Description"),": User-edited documentation that complements or overrides the ingested description"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Change audit stamps"),": Tracks who made edits and when")),(0,n.yg)("p",null,"This separation ensures that manual edits in the UI are preserved and not overwritten by ingestion pipelines."),(0,n.yg)("h3",{id:"ownership"},"Ownership"),(0,n.yg)("p",null,"Like other entities, data jobs support ownership through the ",(0,n.yg)("inlineCode",{parentName:"p"},"ownership")," aspect. Owners can be users or groups with various ownership types (DATAOWNER, PRODUCER, DEVELOPER, etc.). This helps identify who is responsible for maintaining and troubleshooting the job."),(0,n.yg)("h3",{id:"tags-and-glossary-terms"},"Tags and Glossary Terms"),(0,n.yg)("p",null,"Data jobs can be tagged and associated with glossary terms:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Tags")," (",(0,n.yg)("inlineCode",{parentName:"li"},"globalTags")," aspect): Used for categorization, classification, or operational purposes (e.g., PII, critical, deprecated)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Glossary terms")," (",(0,n.yg)("inlineCode",{parentName:"li"},"glossaryTerms")," aspect): Link jobs to business terminology and concepts from your glossary")),(0,n.yg)("h3",{id:"domains-and-applications"},"Domains and Applications"),(0,n.yg)("p",null,"Data jobs can be organized into:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Domains")," (",(0,n.yg)("inlineCode",{parentName:"li"},"domains")," aspect): Business domains or data domains for organizational structure"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Applications")," (",(0,n.yg)("inlineCode",{parentName:"li"},"applications")," aspect): Associated with specific applications or systems")),(0,n.yg)("h3",{id:"structured-properties-and-forms"},"Structured Properties and Forms"),(0,n.yg)("p",null,"Data jobs support:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Structured properties"),": Custom typed properties defined by your organization"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Forms"),": Structured documentation forms for consistency")),(0,n.yg)("h2",{id:"code-examples"},"Code Examples"),(0,n.yg)("h3",{id:"creating-a-data-job"},"Creating a Data Job"),(0,n.yg)("p",null,"The simplest way to create a data job is using the Python SDK v2:"),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Python SDK: Create a basic data job"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'# metadata-ingestion/examples/library/datajob_create_basic.py\nfrom datahub.metadata.urns import DataFlowUrn, DatasetUrn\nfrom datahub.sdk import DataHubClient, DataJob\n\nclient = DataHubClient.from_env()\n\ndatajob = DataJob(\n    name="transform_customer_data",\n    flow_urn=DataFlowUrn(\n        orchestrator="airflow",\n        flow_id="daily_etl_pipeline",\n        cluster="prod",\n    ),\n    description="Transforms raw customer data into analytics-ready format",\n    inlets=[\n        DatasetUrn(platform="postgres", name="raw.customers", env="PROD"),\n        DatasetUrn(platform="postgres", name="raw.addresses", env="PROD"),\n    ],\n    outlets=[\n        DatasetUrn(platform="snowflake", name="analytics.dim_customers", env="PROD"),\n    ],\n)\n\nclient.entities.upsert(datajob)\nprint(f"Created data job: {datajob.urn}")\n\n'))),(0,n.yg)("h3",{id:"adding-tags-terms-and-ownership"},"Adding Tags, Terms, and Ownership"),(0,n.yg)("p",null,"Common metadata can be added to data jobs to enhance discoverability and governance:"),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Python SDK: Add tags, terms, and ownership to a data job"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'# metadata-ingestion/examples/library/datajob_add_tags_terms_ownership.py\nfrom datahub.metadata.urns import (\n    CorpUserUrn,\n    DataFlowUrn,\n    DataJobUrn,\n    GlossaryTermUrn,\n    TagUrn,\n)\nfrom datahub.sdk import DataHubClient\n\nclient = DataHubClient.from_env()\n\ndatajob_urn = DataJobUrn(\n    job_id="transform_customer_data",\n    flow=DataFlowUrn(\n        orchestrator="airflow", flow_id="daily_etl_pipeline", cluster="prod"\n    ),\n)\n\ndatajob = client.entities.get(datajob_urn)\n\ndatajob.add_tag(TagUrn("Critical"))\ndatajob.add_tag(TagUrn("ETL"))\n\ndatajob.add_term(GlossaryTermUrn("CustomerData"))\ndatajob.add_term(GlossaryTermUrn("DataTransformation"))\n\ndatajob.add_owner(CorpUserUrn("data_engineering_team"))\ndatajob.add_owner(CorpUserUrn("john.doe"))\n\nclient.entities.update(datajob)\n\nprint(f"Added tags, terms, and ownership to {datajob_urn}")\n\n'))),(0,n.yg)("h3",{id:"updating-job-properties"},"Updating Job Properties"),(0,n.yg)("p",null,"You can update job properties like descriptions using the low-level APIs:"),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Python SDK: Update data job description"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'# metadata-ingestion/examples/library/datajob_update_description.py\nfrom datahub.sdk import DataFlowUrn, DataHubClient, DataJobUrn\n\nclient = DataHubClient.from_env()\n\ndataflow_urn = DataFlowUrn(\n    orchestrator="airflow", flow_id="daily_etl_pipeline", cluster="prod"\n)\ndatajob_urn = DataJobUrn(flow=dataflow_urn, job_id="transform_customer_data")\n\ndatajob = client.entities.get(datajob_urn)\ndatajob.set_description(\n    "This job performs critical customer data transformation. "\n    "It joins raw customer records with address information and applies "\n    "data quality rules before loading into the analytics warehouse."\n)\n\nclient.entities.update(datajob)\n\nprint(f"Updated description for {datajob_urn}")\n\n'))),(0,n.yg)("h3",{id:"querying-data-job-information"},"Querying Data Job Information"),(0,n.yg)("p",null,"Retrieve data job information via the REST API:"),(0,n.yg)("details",null,(0,n.yg)("summary",null,"REST API: Query a data job"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'# metadata-ingestion/examples/library/datajob_query_rest.py\nimport json\nfrom urllib.parse import quote\n\nimport requests\n\ndatajob_urn = "urn:li:dataJob:(urn:li:dataFlow:(airflow,daily_etl_pipeline,prod),transform_customer_data)"\n\ngms_server = "http://localhost:8080"\nurl = f"{gms_server}/entities/{quote(datajob_urn, safe=\'\')}"\n\nresponse = requests.get(url)\n\nif response.status_code == 200:\n    data = response.json()\n    print(json.dumps(data, indent=2))\n\n    if "aspects" in data:\n        aspects = data["aspects"]\n\n        if "dataJobInfo" in aspects:\n            job_info = aspects["dataJobInfo"]["value"]\n            print(f"\\nJob Name: {job_info.get(\'name\')}")\n            print(f"Description: {job_info.get(\'description\')}")\n            print(f"Type: {job_info.get(\'type\')}")\n\n        if "dataJobInputOutput" in aspects:\n            lineage = aspects["dataJobInputOutput"]["value"]\n            print(f"\\nInput Datasets: {len(lineage.get(\'inputDatasetEdges\', []))}")\n            print(f"Output Datasets: {len(lineage.get(\'outputDatasetEdges\', []))}")\n\n        if "ownership" in aspects:\n            ownership = aspects["ownership"]["value"]\n            print(f"\\nOwners: {len(ownership.get(\'owners\', []))}")\n            for owner in ownership.get("owners", []):\n                print(f"  - {owner.get(\'owner\')} ({owner.get(\'type\')})")\n\n        if "globalTags" in aspects:\n            tags = aspects["globalTags"]["value"]\n            print("\\nTags:")\n            for tag in tags.get("tags", []):\n                print(f"  - {tag.get(\'tag\')}")\nelse:\n    print(f"Failed to retrieve data job: {response.status_code}")\n    print(response.text)\n\n'))),(0,n.yg)("h3",{id:"adding-lineage-to-data-jobs"},"Adding Lineage to Data Jobs"),(0,n.yg)("p",null,"Data jobs are often used to define lineage relationships. See the existing lineage examples:"),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Python SDK: Add lineage using DataJobPatchBuilder"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'from datahub.emitter.mce_builder import (\n    make_data_job_urn,\n    make_dataset_urn,\n    make_schema_field_urn,\n)\nfrom datahub.ingestion.graph.client import DataHubGraph, DataHubGraphConfig\nfrom datahub.metadata.schema_classes import (\n    FineGrainedLineageClass as FineGrainedLineage,\n    FineGrainedLineageDownstreamTypeClass as FineGrainedLineageDownstreamType,\n    FineGrainedLineageUpstreamTypeClass as FineGrainedLineageUpstreamType,\n)\nfrom datahub.specific.datajob import DataJobPatchBuilder\n\n# Create DataHub Client\ndatahub_client = DataHubGraph(DataHubGraphConfig(server="http://localhost:8080"))\n\n# Create DataJob URN\ndatajob_urn = make_data_job_urn(\n    orchestrator="airflow", flow_id="dag_abc", job_id="task_456"\n)\n\n# Create DataJob Patch to Add Lineage\npatch_builder = DataJobPatchBuilder(datajob_urn)\npatch_builder.add_input_dataset(\n    make_dataset_urn(platform="kafka", name="SampleKafkaDataset", env="PROD")\n)\npatch_builder.add_output_dataset(\n    make_dataset_urn(platform="hive", name="SampleHiveDataset", env="PROD")\n)\npatch_builder.add_input_datajob(\n    make_data_job_urn(orchestrator="airflow", flow_id="dag_abc", job_id="task_123")\n)\npatch_builder.add_input_dataset_field(\n    make_schema_field_urn(\n        parent_urn=make_dataset_urn(\n            platform="hive", name="fct_users_deleted", env="PROD"\n        ),\n        field_path="user_id",\n    )\n)\npatch_builder.add_output_dataset_field(\n    make_schema_field_urn(\n        parent_urn=make_dataset_urn(\n            platform="hive", name="fct_users_created", env="PROD"\n        ),\n        field_path="user_id",\n    )\n)\n\n# Update column-level lineage through the Data Job\nlineage1 = FineGrainedLineage(\n    upstreamType=FineGrainedLineageUpstreamType.FIELD_SET,\n    upstreams=[\n        make_schema_field_urn(make_dataset_urn("postgres", "raw_data.users"), "user_id")\n    ],\n    downstreamType=FineGrainedLineageDownstreamType.FIELD,\n    downstreams=[\n        make_schema_field_urn(\n            make_dataset_urn("postgres", "analytics.user_metrics"),\n            "user_id",\n        )\n    ],\n    transformOperation="IDENTITY",\n    confidenceScore=1.0,\n)\npatch_builder.add_fine_grained_lineage(lineage1)\npatch_builder.remove_fine_grained_lineage(lineage1)\n# Replaces all existing fine-grained lineages\npatch_builder.set_fine_grained_lineages([lineage1])\n\npatch_mcps = patch_builder.build()\n\n# Emit DataJob Patch\nfor patch_mcp in patch_mcps:\n    datahub_client.emit(patch_mcp)\n\n'))),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Python SDK: Define fine-grained lineage through a data job"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'import datahub.emitter.mce_builder as builder\nfrom datahub.emitter.mcp import MetadataChangeProposalWrapper\nfrom datahub.emitter.rest_emitter import DatahubRestEmitter\nfrom datahub.metadata.com.linkedin.pegasus2avro.dataset import (\n    FineGrainedLineage,\n    FineGrainedLineageDownstreamType,\n    FineGrainedLineageUpstreamType,\n)\nfrom datahub.metadata.schema_classes import DataJobInputOutputClass\n\n\ndef datasetUrn(tbl):\n    return builder.make_dataset_urn("postgres", tbl)\n\n\ndef fldUrn(tbl, fld):\n    return builder.make_schema_field_urn(datasetUrn(tbl), fld)\n\n\n# Lineage of fields output by a job\n# bar.c1          <-- unknownFunc(bar2.c1, bar4.c1)\n# bar.c2          <-- myfunc(bar3.c2)\n# {bar.c3,bar.c4} <-- unknownFunc(bar2.c2, bar2.c3, bar3.c1)\n# bar.c5          <-- unknownFunc(bar3)\n# {bar.c6,bar.c7} <-- unknownFunc(bar4)\n# bar2.c9 has no upstream i.e. its values are somehow created independently within this job.\n\n# Note that the semantic of the "transformOperation" value is contextual.\n# In above example, it is regarded as some kind of UDF; but it could also be an expression etc.\n\nfineGrainedLineages = [\n    FineGrainedLineage(\n        upstreamType=FineGrainedLineageUpstreamType.FIELD_SET,\n        upstreams=[fldUrn("bar2", "c1"), fldUrn("bar4", "c1")],\n        downstreamType=FineGrainedLineageDownstreamType.FIELD,\n        downstreams=[fldUrn("bar", "c1")],\n    ),\n    FineGrainedLineage(\n        upstreamType=FineGrainedLineageUpstreamType.FIELD_SET,\n        upstreams=[fldUrn("bar3", "c2")],\n        downstreamType=FineGrainedLineageDownstreamType.FIELD,\n        downstreams=[fldUrn("bar", "c2")],\n        confidenceScore=0.8,\n        transformOperation="myfunc",\n    ),\n    FineGrainedLineage(\n        upstreamType=FineGrainedLineageUpstreamType.FIELD_SET,\n        upstreams=[fldUrn("bar2", "c2"), fldUrn("bar2", "c3"), fldUrn("bar3", "c1")],\n        downstreamType=FineGrainedLineageDownstreamType.FIELD_SET,\n        downstreams=[fldUrn("bar", "c3"), fldUrn("bar", "c4")],\n        confidenceScore=0.7,\n    ),\n    FineGrainedLineage(\n        upstreamType=FineGrainedLineageUpstreamType.DATASET,\n        upstreams=[datasetUrn("bar3")],\n        downstreamType=FineGrainedLineageDownstreamType.FIELD,\n        downstreams=[fldUrn("bar", "c5")],\n    ),\n    FineGrainedLineage(\n        upstreamType=FineGrainedLineageUpstreamType.DATASET,\n        upstreams=[datasetUrn("bar4")],\n        downstreamType=FineGrainedLineageDownstreamType.FIELD_SET,\n        downstreams=[fldUrn("bar", "c6"), fldUrn("bar", "c7")],\n    ),\n    FineGrainedLineage(\n        upstreamType=FineGrainedLineageUpstreamType.NONE,\n        upstreams=[],\n        downstreamType=FineGrainedLineageDownstreamType.FIELD,\n        downstreams=[fldUrn("bar2", "c9")],\n    ),\n]\n\n# The lineage of output col bar.c9 is unknown. So there is no lineage for it above.\n# Note that bar2 is an input as well as an output dataset, but some fields are inputs while other fields are outputs.\n\ndataJobInputOutput = DataJobInputOutputClass(\n    inputDatasets=[datasetUrn("bar2"), datasetUrn("bar3"), datasetUrn("bar4")],\n    outputDatasets=[datasetUrn("bar"), datasetUrn("bar2")],\n    inputDatajobs=None,\n    inputDatasetFields=[\n        fldUrn("bar2", "c1"),\n        fldUrn("bar2", "c2"),\n        fldUrn("bar2", "c3"),\n        fldUrn("bar3", "c1"),\n        fldUrn("bar3", "c2"),\n        fldUrn("bar4", "c1"),\n    ],\n    outputDatasetFields=[\n        fldUrn("bar", "c1"),\n        fldUrn("bar", "c2"),\n        fldUrn("bar", "c3"),\n        fldUrn("bar", "c4"),\n        fldUrn("bar", "c5"),\n        fldUrn("bar", "c6"),\n        fldUrn("bar", "c7"),\n        fldUrn("bar", "c9"),\n        fldUrn("bar2", "c9"),\n    ],\n    fineGrainedLineages=fineGrainedLineages,\n)\n\ndataJobLineageMcp = MetadataChangeProposalWrapper(\n    entityUrn=builder.make_data_job_urn("spark", "Flow1", "Task1"),\n    aspect=dataJobInputOutput,\n)\n\n# Create an emitter to the GMS REST API.\nemitter = DatahubRestEmitter("http://localhost:8080")\n\n# Emit metadata!\nemitter.emit_mcp(dataJobLineageMcp)\n\n'))),(0,n.yg)("h2",{id:"integration-points"},"Integration Points"),(0,n.yg)("h3",{id:"relationship-with-dataflow"},"Relationship with DataFlow"),(0,n.yg)("p",null,"Every data job belongs to exactly one ",(0,n.yg)("inlineCode",{parentName:"p"},"dataFlow")," entity, which represents the parent pipeline or workflow. The data flow captures:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"The orchestrator/platform (Airflow, Spark, dbt, etc.)"),(0,n.yg)("li",{parentName:"ul"},"The flow/pipeline/DAG identifier"),(0,n.yg)("li",{parentName:"ul"},"The cluster or environment where it executes")),(0,n.yg)("p",null,"This hierarchical relationship allows DataHub to organize jobs within their workflows and understand the execution context."),(0,n.yg)("h3",{id:"relationship-with-datasets"},"Relationship with Datasets"),(0,n.yg)("p",null,"Data jobs establish lineage by defining:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Consumes")," relationships with input datasets"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Produces")," relationships with output datasets")),(0,n.yg)("p",null,"These relationships are the foundation of DataHub's lineage graph. When a job processes data, it creates a connection between upstream sources and downstream outputs, enabling impact analysis and data discovery."),(0,n.yg)("h3",{id:"relationship-with-dataprocessinstance"},"Relationship with DataProcessInstance"),(0,n.yg)("p",null,"While ",(0,n.yg)("inlineCode",{parentName:"p"},"dataJob")," represents the definition of a processing task, ",(0,n.yg)("inlineCode",{parentName:"p"},"dataProcessInstance")," represents a specific execution or run of that job. Process instances capture:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Runtime information (start time, end time, duration)"),(0,n.yg)("li",{parentName:"ul"},"Status (success, failure, running)"),(0,n.yg)("li",{parentName:"ul"},"Input/output datasets for that specific run"),(0,n.yg)("li",{parentName:"ul"},"Error messages and logs")),(0,n.yg)("p",null,"This separation allows you to track both the static definition of a job and its dynamic runtime behavior."),(0,n.yg)("h3",{id:"graphql-resolvers"},"GraphQL Resolvers"),(0,n.yg)("p",null,"The DataHub GraphQL API provides rich query capabilities for data jobs:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"DataJobType"),": Main type for querying data job information"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"DataJobRunsResolver"),": Resolves execution history and run information"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"DataFlowDataJobsRelationshipsMapper"),": Maps relationships between flows and jobs"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"UpdateLineageResolver"),": Handles lineage updates for jobs")),(0,n.yg)("h3",{id:"ingestion-sources"},"Ingestion Sources"),(0,n.yg)("p",null,"Data jobs are commonly ingested from:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Airflow"),": Tasks and DAGs with lineage extraction"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"dbt"),": Models as data jobs with SQL-based lineage"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Spark"),": Job definitions with dataset dependencies"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Databricks"),": Notebooks and workflows"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Dagster"),": Ops and assets as processing units"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Prefect"),": Tasks and flows"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"AWS Glue"),": ETL jobs"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Azure Data Factory"),": Pipeline activities"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Looker"),": LookML models and derived tables")),(0,n.yg)("p",null,"These connectors automatically extract job definitions, lineage, and metadata from the source systems."),(0,n.yg)("h2",{id:"notable-exceptions"},"Notable Exceptions"),(0,n.yg)("h3",{id:"datahub-ingestion-jobs"},"DataHub Ingestion Jobs"),(0,n.yg)("p",null,"DataHub's own ingestion pipelines are represented as data jobs with special aspects:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"datahubIngestionRunSummary"),": Tracks ingestion run statistics, entities processed, warnings, and errors"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"datahubIngestionCheckpoint"),": Maintains state for incremental ingestion")),(0,n.yg)("p",null,"These aspects are specific to DataHub's internal ingestion framework and are not used for general-purpose data jobs."),(0,n.yg)("h3",{id:"job-status-deprecation"},"Job Status Deprecation"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"status")," field in ",(0,n.yg)("inlineCode",{parentName:"p"},"dataJobInfo")," is deprecated in favor of the ",(0,n.yg)("inlineCode",{parentName:"p"},"dataProcessInstance")," model. Instead of storing job status on the job definition itself, create separate process instance entities for each execution with their own status information. This provides a cleaner separation between job definitions and runtime execution history."),(0,n.yg)("h3",{id:"subtype-usage"},"Subtype Usage"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"subTypes")," aspect allows you to classify jobs into categories:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"SQL jobs"),(0,n.yg)("li",{parentName:"ul"},"Python jobs"),(0,n.yg)("li",{parentName:"ul"},"Notebook jobs"),(0,n.yg)("li",{parentName:"ul"},"Container jobs"),(0,n.yg)("li",{parentName:"ul"},"Custom job types")),(0,n.yg)("p",null,"This helps with filtering and organizing jobs in the UI and API queries."),(0,n.yg)("h2",{id:"technical-reference"},"Technical Reference"),(0,n.yg)("p",null,"For technical details about fields, searchability, and relationships, view the ",(0,n.yg)("strong",{parentName:"p"},"Columns")," tab in DataHub."))}c.isMDXComponent=!0}}]);