"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[29383],{15680:(e,t,a)=>{a.d(t,{xA:()=>u,yg:()=>y});var n=a(96540);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var o=n.createContext({}),d=function(e){var t=n.useContext(o),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},u=function(e){var t=d(e.components);return n.createElement(o.Provider,{value:t},e.children)},p="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,o=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),p=d(a),c=r,y=p["".concat(o,".").concat(c)]||p[c]||m[c]||i;return a?n.createElement(y,s(s({ref:t},u),{},{components:a})):n.createElement(y,s({ref:t},u))}));function y(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,s=new Array(i);s[0]=c;var l={};for(var o in t)hasOwnProperty.call(t,o)&&(l[o]=t[o]);l.originalType=e,l[p]="string"==typeof e?e:r,s[1]=l;for(var d=2;d<i;d++)s[d]=a[d];return n.createElement.apply(null,s)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},18055:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>u,contentTitle:()=>o,default:()=>y,frontMatter:()=>l,metadata:()=>d,toc:()=>p});a(96540);var n=a(15680);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){return t=null!=t?t:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):function(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))})),e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}const l={sidebar_position:1,title:"Dataset",slug:"/generated/metamodel/entities/dataset-datahub",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/generated/metamodel/entities/dataset-datahub.md"},o="Dataset",d={unversionedId:"docs/generated/metamodel/entities/dataset-datahub",id:"docs/generated/metamodel/entities/dataset-datahub",title:"Dataset",description:"The dataset entity is one the most important entities in the metadata model. They represent collections of data that are typically represented as Tables or Views in a database (e.g. BigQuery, Snowflake, Redshift etc.), Streams in a stream-processing environment (Kafka, Pulsar etc.), bundles of data found as Files or Folders in data lake systems (S3, ADLS, etc.).",source:"@site/genDocs/docs/generated/metamodel/entities/dataset-datahub.md",sourceDirName:"docs/generated/metamodel/entities",slug:"/generated/metamodel/entities/dataset-datahub",permalink:"/docs/generated/metamodel/entities/dataset-datahub",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/generated/metamodel/entities/dataset-datahub.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Dataset",slug:"/generated/metamodel/entities/dataset-datahub",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/generated/metamodel/entities/dataset-datahub.md"},sidebar:"overviewSidebar",previous:{title:"Role",permalink:"/docs/generated/metamodel/entities/role"},next:{title:"Dataset",permalink:"/docs/generated/metamodel/entities/dataset"}},u={},p=[{value:"Identity",id:"identity",level:2},{value:"Important Capabilities",id:"important-capabilities",level:2},{value:"Schemas",id:"schemas",level:3},{value:"Field Paths explained",id:"field-paths-explained",level:4},{value:"Tags and Glossary Terms",id:"tags-and-glossary-terms",level:3},{value:"Adding Tags or Glossary Terms at the top-level to a dataset",id:"adding-tags-or-glossary-terms-at-the-top-level-to-a-dataset",level:4},{value:"Adding Tags or Glossary Terms to columns / fields of a dataset",id:"adding-tags-or-glossary-terms-to-columns--fields-of-a-dataset",level:4},{value:"Ownership",id:"ownership",level:3},{value:"Adding Owners",id:"adding-owners",level:4},{value:"Fine-grained lineage",id:"fine-grained-lineage",level:3},{value:"Querying lineage information",id:"querying-lineage-information",level:4},{value:"Documentation, Links etc.",id:"documentation-links-etc",level:3},{value:"Notable Exceptions",id:"notable-exceptions",level:2},{value:"Technical Reference",id:"technical-reference",level:2}],m={toc:p},c="wrapper";function y(e){var{components:t}=e,a=s(e,["components"]);return(0,n.yg)(c,i(function(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{},n=Object.keys(a);"function"==typeof Object.getOwnPropertySymbols&&(n=n.concat(Object.getOwnPropertySymbols(a).filter((function(e){return Object.getOwnPropertyDescriptor(a,e).enumerable})))),n.forEach((function(t){r(e,t,a[t])}))}return e}({},m,a),{components:t,mdxType:"MDXLayout"}),(0,n.yg)("h1",{id:"dataset"},"Dataset"),(0,n.yg)("p",null,"The dataset entity is one the most important entities in the metadata model. They represent collections of data that are typically represented as Tables or Views in a database (e.g. BigQuery, Snowflake, Redshift etc.), Streams in a stream-processing environment (Kafka, Pulsar etc.), bundles of data found as Files or Folders in data lake systems (S3, ADLS, etc.)."),(0,n.yg)("h2",{id:"identity"},"Identity"),(0,n.yg)("p",null,"Datasets are identified by three pieces of information:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"The platform that they belong to: this is the specific data technology that hosts this dataset. Examples are ",(0,n.yg)("inlineCode",{parentName:"li"},"hive"),", ",(0,n.yg)("inlineCode",{parentName:"li"},"bigquery"),", ",(0,n.yg)("inlineCode",{parentName:"li"},"redshift")," etc. See ",(0,n.yg)("a",{parentName:"li",href:"/docs/generated/metamodel/entities/dataplatform"},"dataplatform")," for more details."),(0,n.yg)("li",{parentName:"ul"},"The name of the dataset in the specific platform. Each platform will have a unique way of naming assets within its system. Usually, names are composed by combining the structural elements of the name and separating them by ",(0,n.yg)("inlineCode",{parentName:"li"},"."),". e.g. relational datasets are usually named as ",(0,n.yg)("inlineCode",{parentName:"li"},"<db>.<schema>.<table>"),", except for platforms like MySQL which do not have the concept of a ",(0,n.yg)("inlineCode",{parentName:"li"},"schema"),"; as a result MySQL datasets are named ",(0,n.yg)("inlineCode",{parentName:"li"},"<db>.<table>"),". In cases where the specific platform can have multiple instances (e.g. there are multiple different instances of MySQL databases that have different data assets in them), names can also include instance ids, making the general pattern for a name ",(0,n.yg)("inlineCode",{parentName:"li"},"<platform_instance>.<db>.<schema>.<table>"),"."),(0,n.yg)("li",{parentName:"ul"},"The environment or fabric in which the dataset belongs: this is an additional qualifier available on the identifier, to allow disambiguating datasets that live in Production environments from datasets that live in Non-production environments, such as Staging, QA, etc. The full list of supported environments / fabrics is available in ",(0,n.yg)("a",{parentName:"li",href:"https://raw.githubusercontent.com/datahub-project/datahub/master/li-utils/src/main/pegasus/com/linkedin/common/FabricType.pdl"},"FabricType.pdl"),".")),(0,n.yg)("p",null,"An example of a dataset identifier is ",(0,n.yg)("inlineCode",{parentName:"p"},"urn:li:dataset:(urn:li:dataPlatform:redshift,userdb.public.customer_table,PROD)"),"."),(0,n.yg)("h2",{id:"important-capabilities"},"Important Capabilities"),(0,n.yg)("h3",{id:"schemas"},"Schemas"),(0,n.yg)("p",null,"Datasets support flat and nested schemas. Metadata about schemas are contained in the ",(0,n.yg)("inlineCode",{parentName:"p"},"schemaMetadata")," aspect. Schemas are represented as an array of fields, each identified by a specific field path."),(0,n.yg)("h4",{id:"field-paths-explained"},"Field Paths explained"),(0,n.yg)("p",null,"Fields that are either top-level or expressible unambiguously using a ",(0,n.yg)("inlineCode",{parentName:"p"},".")," based notation can be identified via a v1 path name, whereas fields that are part of a union need further disambiguation using ",(0,n.yg)("inlineCode",{parentName:"p"},"[type=X]")," markers.\nTaking a simple nested schema as described below:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-javascript"},'{\n    "type": "record",\n    "name": "Customer",\n    "fields":[\n        {\n        "type": "record",\n        "name": "address",\n        "fields": [\n            { "name": "zipcode", "type": string},\n            {"name": "street", "type": string}]\n        }],\n}\n')),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"v1 field path: ",(0,n.yg)("inlineCode",{parentName:"li"},"address.zipcode")),(0,n.yg)("li",{parentName:"ul"},"v2 field path: ",(0,n.yg)("inlineCode",{parentName:"li"},'[version=2.0].[type=struct].address.[type=string].zipcode"'),". More examples and a formal specification of a v2 fieldPath can be found ",(0,n.yg)("a",{parentName:"li",href:"/docs/advanced/field-path-spec-v2"},"here"),".")),(0,n.yg)("p",null,"Understanding field paths is important, because they are the identifiers through which tags, terms, documentation on fields are expressed. Besides the type and name of the field, schemas also contain descriptions attached to the individual fields, as well as information about primary and foreign keys."),(0,n.yg)("p",null,"The following code snippet shows you how to add a Schema containing 3 fields to a dataset."),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Python SDK: Add a schema to a dataset"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'from datahub.sdk import DataHubClient, Dataset\n\nclient = DataHubClient.from_env()\n\ndataset = Dataset(\n    platform="hive",\n    name="realestate_db.sales",\n    schema=[\n        # tuples of (field name / field path, data type, description)\n        (\n            "address.zipcode",\n            "varchar(50)",\n            "This is the zipcode of the address. Specified using extended form and limited to addresses in the United States",\n        ),\n        ("address.street", "varchar(100)", "Street corresponding to the address"),\n        ("last_sold_date", "date", "Date of the last sale date for this property"),\n    ],\n)\n\nclient.entities.upsert(dataset)\n\n'))),(0,n.yg)("h3",{id:"tags-and-glossary-terms"},"Tags and Glossary Terms"),(0,n.yg)("p",null,"Datasets can have Tags or Terms attached to them. Read ",(0,n.yg)("a",{parentName:"p",href:"https://medium.com/datahub-project/tags-and-terms-two-powerful-datahub-features-used-in-two-different-scenarios-b5b4791e892e"},"this blog")," to understand the difference between tags and terms so you understand when you should use which."),(0,n.yg)("h4",{id:"adding-tags-or-glossary-terms-at-the-top-level-to-a-dataset"},"Adding Tags or Glossary Terms at the top-level to a dataset"),(0,n.yg)("p",null,"At the top-level, tags are added to datasets using the ",(0,n.yg)("inlineCode",{parentName:"p"},"globalTags")," aspect, while terms are added using the ",(0,n.yg)("inlineCode",{parentName:"p"},"glossaryTerms")," aspect."),(0,n.yg)("p",null,"Here is an example for how to add a tag to a dataset. Note that this involves reading the currently set tags on the dataset and then adding a new one if needed."),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Python SDK: Add a tag to a dataset at the top-level"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'from datahub.sdk import DataHubClient, DatasetUrn, TagUrn\n\nclient = DataHubClient.from_env()\n\ndataset = client.entities.get(DatasetUrn(platform="hive", name="realestate_db.sales"))\ndataset.add_tag(TagUrn("purchase"))\n\nclient.entities.update(dataset)\n\n'))),(0,n.yg)("p",null,"Here is an example of adding a term to a dataset. Note that this involves reading the currently set terms on the dataset and then adding a new one if needed."),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Python SDK: Add a term to a dataset at the top-level"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'from typing import List, Optional, Union\n\nfrom datahub.sdk import DataHubClient, DatasetUrn, GlossaryTermUrn\n\n\ndef add_terms_to_dataset(\n    client: DataHubClient,\n    dataset_urn: DatasetUrn,\n    term_urns: List[Union[GlossaryTermUrn, str]],\n) -> None:\n    """\n    Add glossary terms to a dataset.\n\n    Args:\n        client: DataHub client to use\n        dataset_urn: URN of the dataset to update\n        term_urns: List of term URNs or term names to add\n    """\n    dataset = client.entities.get(dataset_urn)\n\n    for term in term_urns:\n        if isinstance(term, str):\n            resolved_term_urn = client.resolve.term(name=term)\n            dataset.add_term(resolved_term_urn)\n        else:\n            dataset.add_term(term)\n\n    client.entities.update(dataset)\n\n\ndef main(client: Optional[DataHubClient] = None) -> None:\n    """\n    Main function to add terms to dataset example.\n\n    Args:\n        client: Optional DataHub client (for testing). If not provided, creates one from env.\n    """\n    client = client or DataHubClient.from_env()\n\n    dataset_urn = DatasetUrn(platform="hive", name="realestate_db.sales", env="PROD")\n\n    # Add terms using both URN and name resolution\n    add_terms_to_dataset(\n        client=client,\n        dataset_urn=dataset_urn,\n        term_urns=[\n            GlossaryTermUrn("Classification.HighlyConfidential"),\n            "PII",  # Will be resolved by name\n        ],\n    )\n\n\nif __name__ == "__main__":\n    main()\n\n'))),(0,n.yg)("h4",{id:"adding-tags-or-glossary-terms-to-columns--fields-of-a-dataset"},"Adding Tags or Glossary Terms to columns / fields of a dataset"),(0,n.yg)("p",null,"Tags and Terms can also be attached to an individual column (field) of a dataset. These attachments are done via the ",(0,n.yg)("inlineCode",{parentName:"p"},"schemaMetadata")," aspect by ingestion connectors / transformers and via the ",(0,n.yg)("inlineCode",{parentName:"p"},"editableSchemaMetadata")," aspect by the UI.\nThis separation allows the writes from the replication of metadata from the source system to be isolated from the edits made in the UI."),(0,n.yg)("p",null,"Here is an example of how you can add a tag to a field in a dataset using the low-level Python SDK."),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Python SDK: Add a tag to a column (field) of a dataset"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'from datahub.sdk import DataHubClient, DatasetUrn, TagUrn\n\nclient = DataHubClient.from_env()\n\ndataset = client.entities.get(\n    DatasetUrn(platform="hive", name="fct_users_created", env="PROD")\n)\n\ndataset["user_name"].add_tag(TagUrn("deprecated"))\n\nclient.entities.update(dataset)\n\n'))),(0,n.yg)("p",null,"Similarly, here is an example of how you would add a term to a field in a dataset using the low-level Python SDK."),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Python SDK: Add a term to a column (field) of a dataset"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'from datahub.sdk import DataHubClient, DatasetUrn, GlossaryTermUrn\n\nclient = DataHubClient.from_env()\n\ndataset = client.entities.get(\n    DatasetUrn(platform="hive", name="realestate_db.sales", env="PROD")\n)\n\ndataset["address.zipcode"].add_term(GlossaryTermUrn("Classification.Location"))\n\nclient.entities.update(dataset)\n\n'))),(0,n.yg)("h3",{id:"ownership"},"Ownership"),(0,n.yg)("p",null,"Ownership is associated to a dataset using the ",(0,n.yg)("inlineCode",{parentName:"p"},"ownership")," aspect. Owners can be of a few different types, ",(0,n.yg)("inlineCode",{parentName:"p"},"DATAOWNER"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"PRODUCER"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"DEVELOPER"),", ",(0,n.yg)("inlineCode",{parentName:"p"},"CONSUMER"),", etc. See ",(0,n.yg)("a",{parentName:"p",href:"https://raw.githubusercontent.com/datahub-project/datahub/master/metadata-models/src/main/pegasus/com/linkedin/common/OwnershipType.pdl"},"OwnershipType.pdl")," for the full list of ownership types and their meanings. Ownership can be inherited from source systems, or additionally added in DataHub using the UI. Ingestion connectors for sources will automatically set owners when the source system supports it."),(0,n.yg)("h4",{id:"adding-owners"},"Adding Owners"),(0,n.yg)("p",null,"The following script shows you how to add an owner to a dataset using the low-level Python SDK."),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Python SDK: Add an owner to a dataset"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'from datahub.sdk import CorpUserUrn, DataHubClient, DatasetUrn\n\nclient = DataHubClient.from_env()\n\ndataset = client.entities.get(DatasetUrn(platform="hive", name="realestate_db.sales"))\n\n# Add owner with the TECHNICAL_OWNER type\ndataset.add_owner(CorpUserUrn("jdoe"))\n\nclient.entities.update(dataset)\n\n'))),(0,n.yg)("h3",{id:"fine-grained-lineage"},"Fine-grained lineage"),(0,n.yg)("p",null,"Fine-grained lineage at field level can be associated to a dataset in two ways - either directly attached to the ",(0,n.yg)("inlineCode",{parentName:"p"},"upstreamLineage")," aspect of a dataset, or captured as part of the ",(0,n.yg)("inlineCode",{parentName:"p"},"dataJobInputOutput")," aspect of a dataJob."),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Python SDK: Add fine-grained lineage to a dataset"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'from datahub.metadata.urns import DatasetUrn\nfrom datahub.sdk.main_client import DataHubClient\n\nclient = DataHubClient.from_env()\n\nupstream_urn = DatasetUrn(platform="snowflake", name="upstream_table")\ndownstream_urn = DatasetUrn(platform="snowflake", name="downstream_table")\n\ntransformation_text = """\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName("HighValueFilter").getOrCreate()\ndf = spark.read.table("customers")\nhigh_value = df.filter("lifetime_value > 10000")\nhigh_value.write.saveAsTable("high_value_customers")\n"""\n\nclient.lineage.add_lineage(\n    upstream=upstream_urn,\n    downstream=downstream_urn,\n    transformation_text=transformation_text,\n    column_lineage={"id": ["id", "customer_id"]},\n)\n\n# by passing the transformation_text, the query node will be created with the table level lineage.\n# transformation_text can be any transformation logic e.g. a spark job, an airflow DAG, python script, etc.\n# if you have a SQL query, we recommend using add_dataset_lineage_from_sql instead.\n# note that transformation_text itself will not create a column level lineage.\n\n'))),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Python SDK: Add fine-grained lineage to a datajob"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'import datahub.emitter.mce_builder as builder\nfrom datahub.emitter.mcp import MetadataChangeProposalWrapper\nfrom datahub.emitter.rest_emitter import DatahubRestEmitter\nfrom datahub.metadata.com.linkedin.pegasus2avro.dataset import (\n    FineGrainedLineage,\n    FineGrainedLineageDownstreamType,\n    FineGrainedLineageUpstreamType,\n)\nfrom datahub.metadata.schema_classes import DataJobInputOutputClass\n\n\ndef datasetUrn(tbl):\n    return builder.make_dataset_urn("postgres", tbl)\n\n\ndef fldUrn(tbl, fld):\n    return builder.make_schema_field_urn(datasetUrn(tbl), fld)\n\n\n# Lineage of fields output by a job\n# bar.c1          <-- unknownFunc(bar2.c1, bar4.c1)\n# bar.c2          <-- myfunc(bar3.c2)\n# {bar.c3,bar.c4} <-- unknownFunc(bar2.c2, bar2.c3, bar3.c1)\n# bar.c5          <-- unknownFunc(bar3)\n# {bar.c6,bar.c7} <-- unknownFunc(bar4)\n# bar2.c9 has no upstream i.e. its values are somehow created independently within this job.\n\n# Note that the semantic of the "transformOperation" value is contextual.\n# In above example, it is regarded as some kind of UDF; but it could also be an expression etc.\n\nfineGrainedLineages = [\n    FineGrainedLineage(\n        upstreamType=FineGrainedLineageUpstreamType.FIELD_SET,\n        upstreams=[fldUrn("bar2", "c1"), fldUrn("bar4", "c1")],\n        downstreamType=FineGrainedLineageDownstreamType.FIELD,\n        downstreams=[fldUrn("bar", "c1")],\n    ),\n    FineGrainedLineage(\n        upstreamType=FineGrainedLineageUpstreamType.FIELD_SET,\n        upstreams=[fldUrn("bar3", "c2")],\n        downstreamType=FineGrainedLineageDownstreamType.FIELD,\n        downstreams=[fldUrn("bar", "c2")],\n        confidenceScore=0.8,\n        transformOperation="myfunc",\n    ),\n    FineGrainedLineage(\n        upstreamType=FineGrainedLineageUpstreamType.FIELD_SET,\n        upstreams=[fldUrn("bar2", "c2"), fldUrn("bar2", "c3"), fldUrn("bar3", "c1")],\n        downstreamType=FineGrainedLineageDownstreamType.FIELD_SET,\n        downstreams=[fldUrn("bar", "c3"), fldUrn("bar", "c4")],\n        confidenceScore=0.7,\n    ),\n    FineGrainedLineage(\n        upstreamType=FineGrainedLineageUpstreamType.DATASET,\n        upstreams=[datasetUrn("bar3")],\n        downstreamType=FineGrainedLineageDownstreamType.FIELD,\n        downstreams=[fldUrn("bar", "c5")],\n    ),\n    FineGrainedLineage(\n        upstreamType=FineGrainedLineageUpstreamType.DATASET,\n        upstreams=[datasetUrn("bar4")],\n        downstreamType=FineGrainedLineageDownstreamType.FIELD_SET,\n        downstreams=[fldUrn("bar", "c6"), fldUrn("bar", "c7")],\n    ),\n    FineGrainedLineage(\n        upstreamType=FineGrainedLineageUpstreamType.NONE,\n        upstreams=[],\n        downstreamType=FineGrainedLineageDownstreamType.FIELD,\n        downstreams=[fldUrn("bar2", "c9")],\n    ),\n]\n\n# The lineage of output col bar.c9 is unknown. So there is no lineage for it above.\n# Note that bar2 is an input as well as an output dataset, but some fields are inputs while other fields are outputs.\n\ndataJobInputOutput = DataJobInputOutputClass(\n    inputDatasets=[datasetUrn("bar2"), datasetUrn("bar3"), datasetUrn("bar4")],\n    outputDatasets=[datasetUrn("bar"), datasetUrn("bar2")],\n    inputDatajobs=None,\n    inputDatasetFields=[\n        fldUrn("bar2", "c1"),\n        fldUrn("bar2", "c2"),\n        fldUrn("bar2", "c3"),\n        fldUrn("bar3", "c1"),\n        fldUrn("bar3", "c2"),\n        fldUrn("bar4", "c1"),\n    ],\n    outputDatasetFields=[\n        fldUrn("bar", "c1"),\n        fldUrn("bar", "c2"),\n        fldUrn("bar", "c3"),\n        fldUrn("bar", "c4"),\n        fldUrn("bar", "c5"),\n        fldUrn("bar", "c6"),\n        fldUrn("bar", "c7"),\n        fldUrn("bar", "c9"),\n        fldUrn("bar2", "c9"),\n    ],\n    fineGrainedLineages=fineGrainedLineages,\n)\n\ndataJobLineageMcp = MetadataChangeProposalWrapper(\n    entityUrn=builder.make_data_job_urn("spark", "Flow1", "Task1"),\n    aspect=dataJobInputOutput,\n)\n\n# Create an emitter to the GMS REST API.\nemitter = DatahubRestEmitter("http://localhost:8080")\n\n# Emit metadata!\nemitter.emit_mcp(dataJobLineageMcp)\n\n'))),(0,n.yg)("h4",{id:"querying-lineage-information"},"Querying lineage information"),(0,n.yg)("p",null,"The standard ",(0,n.yg)("a",{parentName:"p",href:"/docs/metadata-service/#retrieving-entities"},"GET APIs to retrieve entities")," can be used to fetch the dataset/datajob created by the above example.\nThe response will include the fine-grained lineage information as well."),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Fetch entity snapshot, including fine-grained lineages"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"curl 'http://localhost:8080/entities/urn%3Ali%3Adataset%3A(urn%3Ali%3AdataPlatform%3Apostgres,bar,PROD)'\n")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"curl 'http://localhost:8080/entities/urn%3Ali%3AdataJob%3A(urn%3Ali%3AdataFlow%3A(spark,Flow1,prod),Task1)'\n"))),(0,n.yg)("p",null,"The below queries can be used to find the upstream/downstream datasets/fields of a dataset/datajob."),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Find upstream datasets and fields of a dataset"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},'curl \'http://localhost:8080/relationships?direction=OUTGOING&urn=urn%3Ali%3Adataset%3A(urn%3Ali%3AdataPlatform%3Apostgres,bar,PROD)&types=DownstreamOf\'\n\n{\n    "start": 0,\n    "count": 9,\n    "relationships": [\n        {\n            "type": "DownstreamOf",\n            "entity": "urn:li:dataset:(urn:li:dataPlatform:postgres,bar2,PROD)"\n        },\n        {\n            "type": "DownstreamOf",\n            "entity": "urn:li:dataset:(urn:li:dataPlatform:postgres,bar4,PROD)"\n        },\n        {\n            "type": "DownstreamOf",\n            "entity": "urn:li:dataset:(urn:li:dataPlatform:postgres,bar3,PROD)"\n        },\n        {\n            "type": "DownstreamOf",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar3,PROD),c1)"\n        },\n        {\n            "type": "DownstreamOf",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar2,PROD),c3)"\n        },\n        {\n            "type": "DownstreamOf",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar2,PROD),c2)"\n        },\n        {\n            "type": "DownstreamOf",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar3,PROD),c2)"\n        },\n        {\n            "type": "DownstreamOf",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar4,PROD),c1)"\n        },\n        {\n            "type": "DownstreamOf",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar2,PROD),c1)"\n        }\n    ],\n    "total": 9\n}\n'))),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Find the datasets and fields consumed by a datajob i.e. inputs to a datajob"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},'curl \'http://localhost:8080/relationships?direction=OUTGOING&urn=urn%3Ali%3AdataJob%3A(urn%3Ali%3AdataFlow%3A(spark,Flow1,prod),Task1)&types=Consumes\'\n\n{\n    "start": 0,\n    "count": 9,\n    "relationships": [\n        {\n            "type": "Consumes",\n            "entity": "urn:li:dataset:(urn:li:dataPlatform:postgres,bar4,PROD)"\n        },\n        {\n            "type": "Consumes",\n            "entity": "urn:li:dataset:(urn:li:dataPlatform:postgres,bar3,PROD)"\n        },\n        {\n            "type": "Consumes",\n            "entity": "urn:li:dataset:(urn:li:dataPlatform:postgres,bar2,PROD)"\n        },\n        {\n            "type": "Consumes",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar4,PROD),c1)"\n        },\n        {\n            "type": "Consumes",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar3,PROD),c2)"\n        },\n        {\n            "type": "Consumes",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar3,PROD),c1)"\n        },\n        {\n            "type": "Consumes",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar2,PROD),c3)"\n        },\n        {\n            "type": "Consumes",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar2,PROD),c2)"\n        },\n        {\n            "type": "Consumes",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar2,PROD),c1)"\n        }\n    ],\n    "total": 9\n}\n'))),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Find the datasets and fields produced by a datajob i.e. outputs of a datajob"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},'curl \'http://localhost:8080/relationships?direction=OUTGOING&urn=urn%3Ali%3AdataJob%3A(urn%3Ali%3AdataFlow%3A(spark,Flow1,prod),Task1)&types=Produces\'\n\n{\n    "start": 0,\n    "count": 11,\n    "relationships": [\n        {\n            "type": "Produces",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar2,PROD),c9)"\n        },\n        {\n            "type": "Produces",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar,PROD),c9)"\n        },\n        {\n            "type": "Produces",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar,PROD),c7)"\n        },\n        {\n            "type": "Produces",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar,PROD),c6)"\n        },\n        {\n            "type": "Produces",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar,PROD),c5)"\n        },\n        {\n            "type": "Produces",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar,PROD),c4)"\n        },\n        {\n            "type": "Produces",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar,PROD),c3)"\n        },\n        {\n            "type": "Produces",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar,PROD),c2)"\n        },\n        {\n            "type": "Produces",\n            "entity": "urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,bar,PROD),c1)"\n        },\n        {\n            "type": "Produces",\n            "entity": "urn:li:dataset:(urn:li:dataPlatform:postgres,bar2,PROD)"\n        },\n        {\n            "type": "Produces",\n            "entity": "urn:li:dataset:(urn:li:dataPlatform:postgres,bar,PROD)"\n        }\n    ],\n    "total": 11\n}\n'))),(0,n.yg)("h3",{id:"documentation-links-etc"},"Documentation, Links etc."),(0,n.yg)("p",null,"Documentation for Datasets is available via the ",(0,n.yg)("inlineCode",{parentName:"p"},"datasetProperties")," aspect (typically filled out via ingestion connectors when information is already present in the source system) and via the ",(0,n.yg)("inlineCode",{parentName:"p"},"editableDatasetProperties")," aspect (filled out via the UI typically)"),(0,n.yg)("p",null,"Links that contain more knowledge about the dataset (e.g. links to Confluence pages) can be added via the ",(0,n.yg)("inlineCode",{parentName:"p"},"institutionalMemory")," aspect."),(0,n.yg)("p",null,"Here is a simple script that shows you how to add documentation for a dataset including some links to pages using the low-level Python SDK."),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Python SDK: Add documentation, links to a dataset"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'from datahub.sdk import DataHubClient, DatasetUrn\n\nclient = DataHubClient.from_env()\n\ndataset = client.entities.get(DatasetUrn(platform="hive", name="realestate_db.sales"))\n\n# Add dataset documentation\ndocumentation = """## The Real Estate Sales Dataset\nThis is a really important Dataset that contains all the relevant information about sales that have happened organized by address.\n"""\ndataset.set_description(documentation)\n\n# Add link to institutional memory\ndataset.add_link(\n    (\n        "https://wikipedia.com/real_estate",\n        "This is the definition of what real estate means",  # link description\n    )\n)\n\nclient.entities.update(dataset)\n\n'))),(0,n.yg)("h2",{id:"notable-exceptions"},"Notable Exceptions"),(0,n.yg)("p",null,"The following overloaded uses of the Dataset entity exist for convenience, but will likely move to fully modeled entity types in the future."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"OpenAPI endpoints: the GET API of OpenAPI endpoints are currently modeled as Datasets, but should really be modeled as a Service/API entity once this is created in the metadata model."),(0,n.yg)("li",{parentName:"ul"},"DataHub's Logical Entities (e.g.. Dataset, Chart, Dashboard) are represented as Datasets, with sub-type Entity. These should really be modeled as Entities in a logical ER model once this is created in the metadata model.")),(0,n.yg)("h2",{id:"technical-reference"},"Technical Reference"),(0,n.yg)("p",null,"For technical details about fields, searchability, and relationships, view the ",(0,n.yg)("strong",{parentName:"p"},"Columns")," tab in DataHub."))}y.isMDXComponent=!0}}]);