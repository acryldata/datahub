"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[38342],{15680:(e,t,a)=>{a.d(t,{xA:()=>d,yg:()=>m});var n=a(96540);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},d=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},g="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),g=p(a),u=r,m=g["".concat(l,".").concat(u)]||g[u]||c[u]||i;return a?n.createElement(m,o(o({ref:t},d),{},{components:a})):n.createElement(m,o({ref:t},d))}));function m(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[g]="string"==typeof e?e:r,o[1]=s;for(var p=2;p<i;p++)o[p]=a[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},92074:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>l,default:()=>m,frontMatter:()=>s,metadata:()=>p,toc:()=>g});a(96540);var n=a(15680);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){return t=null!=t?t:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):function(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))})),e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}const s={sidebar_position:4,title:"DataProcess",slug:"/generated/metamodel/entities/dataprocess-datahub",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/generated/metamodel/entities/dataProcess-datahub.md"},l="DataProcess",p={unversionedId:"docs/generated/metamodel/entities/dataProcess-datahub",id:"docs/generated/metamodel/entities/dataProcess-datahub",title:"DataProcess",description:"DEPRECATED: This entity is deprecated and should not be used for new implementations.",source:"@site/genDocs/docs/generated/metamodel/entities/dataProcess-datahub.md",sourceDirName:"docs/generated/metamodel/entities",slug:"/generated/metamodel/entities/dataprocess-datahub",permalink:"/docs/generated/metamodel/entities/dataprocess-datahub",draft:!1,editUrl:"https://github.com/datahub-project/datahub/blob/master/docs/generated/metamodel/entities/dataProcess-datahub.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4,title:"DataProcess",slug:"/generated/metamodel/entities/dataprocess-datahub",custom_edit_url:"https://github.com/datahub-project/datahub/blob/master/docs/generated/metamodel/entities/dataProcess-datahub.md"},sidebar:"overviewSidebar",previous:{title:"DataFlow",permalink:"/docs/generated/metamodel/entities/dataflow"},next:{title:"DataProcess",permalink:"/docs/generated/metamodel/entities/dataprocess"}},d={},g=[{value:"Deprecation Notice",id:"deprecation-notice",level:2},{value:"Why was it deprecated?",id:"why-was-it-deprecated",level:3},{value:"Identity (Historical Reference)",id:"identity-historical-reference",level:2},{value:"Example URNs",id:"example-urns",level:3},{value:"Important Capabilities (Historical Reference)",id:"important-capabilities-historical-reference",level:2},{value:"DataProcessInfo Aspect",id:"dataprocessinfo-aspect",level:3},{value:"Common Aspects",id:"common-aspects",level:3},{value:"Migration Guide",id:"migration-guide",level:2},{value:"When to use DataFlow vs DataJob",id:"when-to-use-dataflow-vs-datajob",level:3},{value:"Conceptual Mapping",id:"conceptual-mapping",level:3},{value:"Migration Steps",id:"migration-steps",level:3},{value:"Migration Examples",id:"migration-examples",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"Querying Existing DataProcess Entities",id:"querying-existing-dataprocess-entities",level:3},{value:"Creating Equivalent DataFlow and DataJob (Recommended)",id:"creating-equivalent-dataflow-and-datajob-recommended",level:3},{value:"Complete Migration Example",id:"complete-migration-example",level:3},{value:"Integration Points",id:"integration-points",level:2},{value:"Historical Usage",id:"historical-usage",level:3},{value:"Modern Replacements",id:"modern-replacements",level:3},{value:"DataProcessInstance",id:"dataprocessinstance",level:3},{value:"Notable Exceptions",id:"notable-exceptions",level:2},{value:"Timeline for Removal",id:"timeline-for-removal",level:3},{value:"Reading Existing Data",id:"reading-existing-data",level:3},{value:"No New Writes Recommended",id:"no-new-writes-recommended",level:3},{value:"Upgrade Path",id:"upgrade-path",level:3},{value:"GraphQL API",id:"graphql-api",level:3},{value:"Additional Resources",id:"additional-resources",level:2},{value:"Technical Reference",id:"technical-reference",level:2}],c={toc:g},u="wrapper";function m(e){var{components:t}=e,a=o(e,["components"]);return(0,n.yg)(u,i(function(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{},n=Object.keys(a);"function"==typeof Object.getOwnPropertySymbols&&(n=n.concat(Object.getOwnPropertySymbols(a).filter((function(e){return Object.getOwnPropertyDescriptor(a,e).enumerable})))),n.forEach((function(t){r(e,t,a[t])}))}return e}({},c,a),{components:t,mdxType:"MDXLayout"}),(0,n.yg)("h1",{id:"dataprocess"},"DataProcess"),(0,n.yg)("blockquote",null,(0,n.yg)("p",{parentName:"blockquote"},(0,n.yg)("strong",{parentName:"p"},"DEPRECATED"),": This entity is deprecated and should not be used for new implementations."),(0,n.yg)("p",{parentName:"blockquote"},(0,n.yg)("strong",{parentName:"p"},"Use ",(0,n.yg)("a",{parentName:"strong",href:"/docs/generated/metamodel/entities/dataflow"},"dataFlow")," and ",(0,n.yg)("a",{parentName:"strong",href:"/docs/generated/metamodel/entities/datajob"},"dataJob")," instead.")),(0,n.yg)("p",{parentName:"blockquote"},"The ",(0,n.yg)("inlineCode",{parentName:"p"},"dataProcess")," entity was an early attempt to model data processing tasks but has been superseded by the more robust and flexible ",(0,n.yg)("inlineCode",{parentName:"p"},"dataFlow")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"dataJob")," entities which better represent the hierarchical nature of modern data pipelines.")),(0,n.yg)("h2",{id:"deprecation-notice"},"Deprecation Notice"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"dataProcess")," entity was deprecated to provide a clearer separation between:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"DataFlow"),": Represents the overall pipeline/workflow (e.g., an Airflow DAG, dbt project, Spark application)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"DataJob"),": Represents individual tasks within a pipeline (e.g., an Airflow task, dbt model, Spark job)")),(0,n.yg)("p",null,"This two-level hierarchy better matches how modern orchestration systems organize data processing work and provides more flexibility for lineage tracking, ownership assignment, and operational monitoring."),(0,n.yg)("h3",{id:"why-was-it-deprecated"},"Why was it deprecated?"),(0,n.yg)("p",null,"The original ",(0,n.yg)("inlineCode",{parentName:"p"},"dataProcess")," entity had several limitations:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"No hierarchical structure"),": It couldn't represent the relationship between a pipeline and its constituent tasks"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Limited orchestrator support"),": The flat structure didn't map well to DAG-based orchestration platforms like Airflow, Prefect, or Dagster"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Unclear semantics"),": It was ambiguous whether a dataProcess represented a whole pipeline or a single task"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Poor lineage modeling"),": Without task-level granularity, lineage relationships were less precise")),(0,n.yg)("p",null,"The new ",(0,n.yg)("inlineCode",{parentName:"p"},"dataFlow")," and ",(0,n.yg)("inlineCode",{parentName:"p"},"dataJob")," model addresses these limitations by providing a clear parent-child relationship that mirrors real-world data processing architectures."),(0,n.yg)("h2",{id:"identity-historical-reference"},"Identity (Historical Reference)"),(0,n.yg)("p",null,"DataProcess entities were identified by three components:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Name"),": The process name (typically an ETL job name)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Orchestrator"),": The workflow management platform (e.g., ",(0,n.yg)("inlineCode",{parentName:"li"},"airflow"),", ",(0,n.yg)("inlineCode",{parentName:"li"},"azkaban"),")"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Origin (Fabric)"),": The environment where the process runs (PROD, DEV, etc.)")),(0,n.yg)("p",null,"The URN structure was:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"urn:li:dataProcess:(<name>,<orchestrator>,<origin>)\n")),(0,n.yg)("h3",{id:"example-urns"},"Example URNs"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"urn:li:dataProcess:(customer_etl_job,airflow,PROD)\nurn:li:dataProcess:(sales_aggregation,azkaban,DEV)\n")),(0,n.yg)("h2",{id:"important-capabilities-historical-reference"},"Important Capabilities (Historical Reference)"),(0,n.yg)("h3",{id:"dataprocessinfo-aspect"},"DataProcessInfo Aspect"),(0,n.yg)("p",null,"The ",(0,n.yg)("inlineCode",{parentName:"p"},"dataProcessInfo")," aspect captured inputs and outputs of the process:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Inputs"),": Array of dataset URNs consumed by the process"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Outputs"),": Array of dataset URNs produced by the process")),(0,n.yg)("p",null,'This established basic lineage relationships through "Consumes" relationships with datasets.'),(0,n.yg)("h3",{id:"common-aspects"},"Common Aspects"),(0,n.yg)("p",null,"Like other entities, dataProcess supported:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Ownership"),": Assigning owners to processes"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Status"),": Marking processes as removed"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Global Tags"),": Categorization and classification"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Institutional Memory"),": Links to documentation")),(0,n.yg)("h2",{id:"migration-guide"},"Migration Guide"),(0,n.yg)("h3",{id:"when-to-use-dataflow-vs-datajob"},"When to use DataFlow vs DataJob"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Use DataFlow when representing:")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Airflow DAGs"),(0,n.yg)("li",{parentName:"ul"},"dbt projects"),(0,n.yg)("li",{parentName:"ul"},"Prefect flows"),(0,n.yg)("li",{parentName:"ul"},"Dagster pipelines"),(0,n.yg)("li",{parentName:"ul"},"Azkaban workflows"),(0,n.yg)("li",{parentName:"ul"},"Any container of related data processing tasks")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Use DataJob when representing:")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Airflow tasks within a DAG"),(0,n.yg)("li",{parentName:"ul"},"dbt models within a project"),(0,n.yg)("li",{parentName:"ul"},"Prefect tasks within a flow"),(0,n.yg)("li",{parentName:"ul"},"Dagster ops/assets within a pipeline"),(0,n.yg)("li",{parentName:"ul"},"Individual processing steps")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Use both together:")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Create a DataFlow for the pipeline"),(0,n.yg)("li",{parentName:"ul"},"Create DataJobs for each task within that pipeline"),(0,n.yg)("li",{parentName:"ul"},"Link DataJobs to their parent DataFlow")),(0,n.yg)("h3",{id:"conceptual-mapping"},"Conceptual Mapping"),(0,n.yg)("table",null,(0,n.yg)("thead",{parentName:"table"},(0,n.yg)("tr",{parentName:"thead"},(0,n.yg)("th",{parentName:"tr",align:null},"DataProcess Concept"),(0,n.yg)("th",{parentName:"tr",align:null},"New Model Equivalent"),(0,n.yg)("th",{parentName:"tr",align:null},"Notes"))),(0,n.yg)("tbody",{parentName:"table"},(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Process with tasks"),(0,n.yg)("td",{parentName:"tr",align:null},"DataFlow + DataJobs"),(0,n.yg)("td",{parentName:"tr",align:null},"Split into two entities")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Process name"),(0,n.yg)("td",{parentName:"tr",align:null},"DataFlow flowId"),(0,n.yg)("td",{parentName:"tr",align:null},"Becomes the parent identifier")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Single-step process"),(0,n.yg)("td",{parentName:"tr",align:null},"DataFlow + 1 DataJob"),(0,n.yg)("td",{parentName:"tr",align:null},"Still requires both entities")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Orchestrator"),(0,n.yg)("td",{parentName:"tr",align:null},"DataFlow orchestrator"),(0,n.yg)("td",{parentName:"tr",align:null},"Same concept, better modeling")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Origin/Fabric"),(0,n.yg)("td",{parentName:"tr",align:null},"DataFlow cluster"),(0,n.yg)("td",{parentName:"tr",align:null},"Often matches environment")),(0,n.yg)("tr",{parentName:"tbody"},(0,n.yg)("td",{parentName:"tr",align:null},"Inputs/Outputs"),(0,n.yg)("td",{parentName:"tr",align:null},"DataJob dataJobInputOutput"),(0,n.yg)("td",{parentName:"tr",align:null},"Moved to job level for precision")))),(0,n.yg)("h3",{id:"migration-steps"},"Migration Steps"),(0,n.yg)("p",null,"To migrate from ",(0,n.yg)("inlineCode",{parentName:"p"},"dataProcess")," to ",(0,n.yg)("inlineCode",{parentName:"p"},"dataFlow"),"/",(0,n.yg)("inlineCode",{parentName:"p"},"dataJob"),":"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Identify your process structure"),": Determine if your dataProcess represents a pipeline (has multiple steps) or a single task")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Create a DataFlow"),": This represents the overall pipeline/workflow"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"Use the same orchestrator value"),(0,n.yg)("li",{parentName:"ul"},"Use the process name as the flow ID"),(0,n.yg)("li",{parentName:"ul"},"Use a cluster identifier (often matches the origin/fabric)"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Create DataJob(s)"),": Create one or more jobs within the flow"),(0,n.yg)("ul",{parentName:"li"},(0,n.yg)("li",{parentName:"ul"},"For single-step processes: create one job named after the process"),(0,n.yg)("li",{parentName:"ul"},"For multi-step processes: create a job for each step"),(0,n.yg)("li",{parentName:"ul"},"Link each job to its parent DataFlow"))),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Migrate lineage"),": Move input/output dataset relationships from the process level to the job level")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Migrate metadata"),": Transfer ownership, tags, and documentation to the appropriate entity (typically the DataFlow for pipeline-level metadata, or specific DataJobs for task-level metadata)"))),(0,n.yg)("h3",{id:"migration-examples"},"Migration Examples"),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Example 1: Simple single-task process")),(0,n.yg)("p",null,"Old dataProcess:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"urn:li:dataProcess:(daily_report,airflow,PROD)\n")),(0,n.yg)("p",null,"New structure:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"DataFlow: urn:li:dataFlow:(airflow,daily_report,prod)\nDataJob:  urn:li:dataJob:(urn:li:dataFlow:(airflow,daily_report,prod),daily_report_task)\n")),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Example 2: Multi-step ETL pipeline")),(0,n.yg)("p",null,"Old dataProcess:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"urn:li:dataProcess:(customer_pipeline,airflow,PROD)\n")),(0,n.yg)("p",null,"New structure:"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre"},"DataFlow: urn:li:dataFlow:(airflow,customer_pipeline,prod)\nDataJob:  urn:li:dataJob:(urn:li:dataFlow:(airflow,customer_pipeline,prod),extract_customers)\nDataJob:  urn:li:dataJob:(urn:li:dataFlow:(airflow,customer_pipeline,prod),transform_customers)\nDataJob:  urn:li:dataJob:(urn:li:dataFlow:(airflow,customer_pipeline,prod),load_customers)\n")),(0,n.yg)("h2",{id:"code-examples"},"Code Examples"),(0,n.yg)("h3",{id:"querying-existing-dataprocess-entities"},"Querying Existing DataProcess Entities"),(0,n.yg)("p",null,"If you need to query existing dataProcess entities for migration purposes:"),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Python SDK: Query a dataProcess entity"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'"""\nExample: Query an existing (deprecated) dataProcess entity for migration purposes.\n\nThis example shows how to read a deprecated dataProcess entity from DataHub\nto understand its structure before migrating it to dataFlow and dataJob entities.\n\nNote: This is only for reading existing data. Do NOT create new dataProcess entities.\nUse dataFlow and dataJob instead for all new implementations.\n"""\n\nfrom datahub.emitter.rest_emitter import DatahubRestEmitter\n\n# Create emitter to read from DataHub\nemitter = DatahubRestEmitter(gms_server="http://localhost:8080")\n\n# URN of the deprecated dataProcess entity to query\ndataprocess_urn = "urn:li:dataProcess:(customer_etl_job,airflow,PROD)"\n\n# Fetch the entity using the REST API\ntry:\n    entity = emitter._session.get(\n        f"{emitter._gms_server}/entities/{dataprocess_urn}"\n    ).json()\n\n    print(f"Found dataProcess: {dataprocess_urn}")\n    print("\\n=== Entity Aspects ===")\n\n    # Extract key information for migration\n    if "aspects" in entity:\n        aspects = entity["aspects"]\n\n        # Key aspect (identity)\n        if "dataProcessKey" in aspects:\n            key = aspects["dataProcessKey"]\n            print("\\nIdentity:")\n            print(f"  Name: {key.get(\'name\')}")\n            print(f"  Orchestrator: {key.get(\'orchestrator\')}")\n            print(f"  Origin (Fabric): {key.get(\'origin\')}")\n\n        # Core process information\n        if "dataProcessInfo" in aspects:\n            info = aspects["dataProcessInfo"]\n            print("\\nProcess Info:")\n            if "inputs" in info:\n                print(f"  Input Datasets: {len(info[\'inputs\'])}")\n                for inp in info["inputs"]:\n                    print(f"    - {inp}")\n            if "outputs" in info:\n                print(f"  Output Datasets: {len(info[\'outputs\'])}")\n                for out in info["outputs"]:\n                    print(f"    - {out}")\n\n        # Ownership information\n        if "ownership" in aspects:\n            ownership = aspects["ownership"]\n            print("\\nOwnership:")\n            for owner in ownership.get("owners", []):\n                print(f"  - {owner[\'owner\']} (type: {owner.get(\'type\', \'UNKNOWN\')})")\n\n        # Tags\n        if "globalTags" in aspects:\n            tags = aspects["globalTags"]\n            print("\\nTags:")\n            for tag in tags.get("tags", []):\n                print(f"  - {tag[\'tag\']}")\n\n        # Status\n        if "status" in aspects:\n            status = aspects["status"]\n            print(f"\\nStatus: {status.get(\'removed\', False)}")\n\n    print("\\n=== Migration Recommendation ===")\n    print("Replace this dataProcess with:")\n    print(\n        f"  DataFlow URN: urn:li:dataFlow:({key.get(\'orchestrator\')},{key.get(\'name\')},{key.get(\'origin\', \'PROD\').lower()})"\n    )\n    print(\n        f"  DataJob URN: urn:li:dataJob:(urn:li:dataFlow:({key.get(\'orchestrator\')},{key.get(\'name\')},{key.get(\'origin\', \'PROD\').lower()}),main_task)"\n    )\n    print("\\nSee dataprocess_migrate_to_flow_job.py for migration code examples.")\n\nexcept Exception as e:\n    print(f"Error querying dataProcess: {e}")\n    print("\\nThis is expected if the entity doesn\'t exist.")\n    print("DataProcess is deprecated - use dataFlow and dataJob instead.")\n\n'))),(0,n.yg)("h3",{id:"creating-equivalent-dataflow-and-datajob-recommended"},"Creating Equivalent DataFlow and DataJob (Recommended)"),(0,n.yg)("p",null,"Instead of using dataProcess, create the modern equivalent:"),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Python SDK: Create DataFlow and DataJob to replace dataProcess"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'"""\nExample: Migrate from deprecated dataProcess to modern dataFlow and dataJob entities.\n\nThis example shows how to create the modern dataFlow and dataJob entities\nthat replace the deprecated dataProcess entity.\n\nThe dataProcess entity used a flat structure:\n  dataProcess -> inputs/outputs\n\nThe new model uses a hierarchical structure:\n  dataFlow (pipeline) -> dataJob (task) -> inputs/outputs\n\nThis provides better organization and more precise lineage tracking.\n"""\n\nfrom datahub.metadata.urns import DatasetUrn\nfrom datahub.sdk import DataFlow, DataHubClient, DataJob\n\nclient = DataHubClient.from_env()\n\n# Old dataProcess would have been:\n# urn:li:dataProcess:(customer_etl_job,airflow,PROD)\n# with inputs and outputs at the process level\n\n# New approach: Create a DataFlow for the pipeline\ndataflow = DataFlow(\n    platform="airflow",  # Same as the old \'orchestrator\' field\n    name="customer_etl_job",  # Same as the old \'name\' field\n    platform_instance="prod",  # Based on old \'origin\' field\n    description="ETL pipeline for customer data processing",\n)\n\n# Create DataJob(s) within the flow\n# For a simple single-task process, create one job\n# For complex multi-step processes, create multiple jobs\ndatajob = DataJob(\n    name="customer_etl_task",  # Task name within the flow\n    flow=dataflow,  # Link to parent flow\n    description="Main ETL task for customer data",\n    # Inputs and outputs now live at the job level for precise lineage\n    inlets=[\n        DatasetUrn(platform="mysql", name="raw_db.customers", env="PROD"),\n        DatasetUrn(platform="mysql", name="raw_db.orders", env="PROD"),\n    ],\n    outlets=[\n        DatasetUrn(platform="postgres", name="analytics.customer_summary", env="PROD"),\n    ],\n)\n\n# Upsert both entities\nclient.entities.upsert(dataflow)\nclient.entities.upsert(datajob)\n\nprint("Successfully migrated from dataProcess to dataFlow + dataJob!")\nprint(f"DataFlow URN: {dataflow.urn}")\nprint(f"DataJob URN: {datajob.urn}")\nprint("\\nKey improvements over dataProcess:")\nprint("- Clear separation between pipeline (DataFlow) and task (DataJob)")\nprint("- Support for multi-step pipelines with multiple DataJobs")\nprint("- More precise lineage at the task level")\nprint("- Better integration with modern orchestration platforms")\n\n'))),(0,n.yg)("h3",{id:"complete-migration-example"},"Complete Migration Example"),(0,n.yg)("details",null,(0,n.yg)("summary",null,"Python SDK: Full migration from dataProcess to dataFlow/dataJob"),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-python"},'"""\nExample: Complete migration from dataProcess to dataFlow/dataJob with metadata preservation.\n\nThis example demonstrates a full migration path that:\n1. Reads an existing deprecated dataProcess entity\n2. Extracts all its metadata (inputs, outputs, ownership, tags)\n3. Creates equivalent dataFlow and dataJob entities\n4. Preserves all metadata relationships\n\nUse this as a template for migrating multiple dataProcess entities in bulk.\n"""\n\nfrom datahub.emitter.mcp import MetadataChangeProposalWrapper\nfrom datahub.emitter.rest_emitter import DatahubRestEmitter\nfrom datahub.metadata.schema_classes import (\n    GlobalTagsClass,\n    OwnerClass,\n    OwnershipClass,\n    OwnershipTypeClass,\n    TagAssociationClass,\n)\nfrom datahub.sdk import DataFlow, DataHubClient, DataJob\n\n# Initialize clients\nrest_emitter = DatahubRestEmitter(gms_server="http://localhost:8080")\nclient = DataHubClient.from_env()\n\n# Step 1: Define the dataProcess to migrate\nold_dataprocess_urn = "urn:li:dataProcess:(sales_pipeline,airflow,PROD)"\n\nprint(f"Migrating: {old_dataprocess_urn}")\n\ntry:\n    # Step 2: Fetch the existing dataProcess entity\n    entity = rest_emitter._session.get(\n        f"{rest_emitter._gms_server}/entities/{old_dataprocess_urn}"\n    ).json()\n\n    aspects = entity.get("aspects", {})\n\n    # Extract identity information\n    key = aspects.get("dataProcessKey", {})\n    name = key.get("name", "unknown_process")\n    orchestrator = key.get("orchestrator", "unknown")\n    origin = key.get("origin", "PROD")\n\n    # Extract process info\n    process_info = aspects.get("dataProcessInfo", {})\n    input_datasets = process_info.get("inputs", [])\n    output_datasets = process_info.get("outputs", [])\n\n    # Extract ownership\n    ownership_aspect = aspects.get("ownership", {})\n    owners = ownership_aspect.get("owners", [])\n\n    # Extract tags\n    tags_aspect = aspects.get("globalTags", {})\n    tags = tags_aspect.get("tags", [])\n\n    print("\\n=== Extracted Metadata ===")\n    print(f"Name: {name}")\n    print(f"Orchestrator: {orchestrator}")\n    print(f"Environment: {origin}")\n    print(f"Inputs: {len(input_datasets)} datasets")\n    print(f"Outputs: {len(output_datasets)} datasets")\n    print(f"Owners: {len(owners)}")\n    print(f"Tags: {len(tags)}")\n\n    # Step 3: Create the new DataFlow\n    dataflow = DataFlow(\n        platform=orchestrator,\n        name=name,\n        platform_instance=origin.lower(),\n        description=f"Migrated from dataProcess {name}",\n    )\n\n    # Step 4: Create the DataJob(s)\n    # For simplicity, creating one job. In practice, you might split into multiple jobs.\n    datajob = DataJob(\n        name=f"{name}_main",\n        flow=dataflow,\n        description=f"Main task for {name}",\n        inlets=[inp for inp in input_datasets],  # These should be dataset URNs\n        outlets=[out for out in output_datasets],  # These should be dataset URNs\n    )\n\n    # Step 5: Upsert the entities\n    client.entities.upsert(dataflow)\n    client.entities.upsert(datajob)\n\n    print("\\n=== Created New Entities ===")\n    print(f"DataFlow: {dataflow.urn}")\n    print(f"DataJob: {datajob.urn}")\n\n    # Step 6: Migrate ownership to DataFlow\n    if owners:\n        ownership_to_add = OwnershipClass(\n            owners=[\n                OwnerClass(\n                    owner=owner.get("owner"),\n                    type=getattr(OwnershipTypeClass, owner.get("type", "DATAOWNER")),\n                )\n                for owner in owners\n            ]\n        )\n        rest_emitter.emit_mcp(\n            MetadataChangeProposalWrapper(\n                entityUrn=str(dataflow.urn),\n                aspect=ownership_to_add,\n            )\n        )\n        print(f"Migrated {len(owners)} owner(s) to DataFlow")\n\n    # Step 7: Migrate tags to DataFlow\n    if tags:\n        tags_to_add = GlobalTagsClass(\n            tags=[TagAssociationClass(tag=tag.get("tag")) for tag in tags]\n        )\n        rest_emitter.emit_mcp(\n            MetadataChangeProposalWrapper(\n                entityUrn=str(dataflow.urn),\n                aspect=tags_to_add,\n            )\n        )\n        print(f"Migrated {len(tags)} tag(s) to DataFlow")\n\n    print("\\n=== Migration Complete ===")\n    print("Next steps:")\n    print("1. Verify the new entities in DataHub UI")\n    print("2. Update any downstream systems to reference the new URNs")\n    print("3. Consider soft-deleting the old dataProcess entity")\n\nexcept Exception as e:\n    print(f"Error during migration: {e}")\n    print("\\nCommon issues:")\n    print("- DataProcess entity doesn\'t exist (already migrated or never created)")\n    print("- Network connectivity to DataHub GMS")\n    print("- Permission issues writing to DataHub")\n\n'))),(0,n.yg)("h2",{id:"integration-points"},"Integration Points"),(0,n.yg)("h3",{id:"historical-usage"},"Historical Usage"),(0,n.yg)("p",null,"The dataProcess entity was previously used by:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Early ingestion connectors"),": Original Airflow, Azkaban connectors before they migrated to dataFlow/dataJob"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Custom integrations"),": User-built integrations that haven't been updated"),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("strong",{parentName:"li"},"Legacy metadata"),": Historical data in existing DataHub instances")),(0,n.yg)("h3",{id:"modern-replacements"},"Modern Replacements"),(0,n.yg)("p",null,"All modern DataHub connectors use dataFlow and dataJob:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Airflow"),": DAGs \u2192 DataFlow, Tasks \u2192 DataJob"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"dbt"),": Projects \u2192 DataFlow, Models \u2192 DataJob"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Prefect"),": Flows \u2192 DataFlow, Tasks \u2192 DataJob"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Dagster"),": Pipelines \u2192 DataFlow, Ops/Assets \u2192 DataJob"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Fivetran"),": Connectors \u2192 DataFlow, Sync operations \u2192 DataJob"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"AWS Glue"),": Jobs \u2192 DataFlow, Steps \u2192 DataJob"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Azure Data Factory"),": Pipelines \u2192 DataFlow, Activities \u2192 DataJob")),(0,n.yg)("h3",{id:"dataprocessinstance"},"DataProcessInstance"),(0,n.yg)("p",null,"Note that ",(0,n.yg)("inlineCode",{parentName:"p"},"dataProcessInstance")," is ",(0,n.yg)("strong",{parentName:"p"},"NOT deprecated"),". It represents a specific execution/run of either:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"A dataJob (recommended)"),(0,n.yg)("li",{parentName:"ul"},"A legacy dataProcess (for backward compatibility)")),(0,n.yg)("p",null,"DataProcessInstance continues to be used for tracking pipeline run history, status, and runtime information."),(0,n.yg)("h2",{id:"notable-exceptions"},"Notable Exceptions"),(0,n.yg)("h3",{id:"timeline-for-removal"},"Timeline for Removal"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Deprecated"),": Early 2021 (with introduction of dataFlow/dataJob)"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Status"),": Still exists in the entity registry for backward compatibility"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Current State"),": No active ingestion sources create dataProcess entities"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Removal"),": No specific timeline, maintained for existing data")),(0,n.yg)("h3",{id:"reading-existing-data"},"Reading Existing Data"),(0,n.yg)("p",null,"The dataProcess entity remains readable through all DataHub APIs for backward compatibility. Existing dataProcess entities in your instance will continue to function and display in the UI."),(0,n.yg)("h3",{id:"no-new-writes-recommended"},"No New Writes Recommended"),(0,n.yg)("p",null,"While technically possible to create new dataProcess entities, it is ",(0,n.yg)("strong",{parentName:"p"},"strongly discouraged"),". All new integrations should use dataFlow and dataJob."),(0,n.yg)("h3",{id:"upgrade-path"},"Upgrade Path"),(0,n.yg)("p",null,"There is no automatic migration tool. Organizations with significant dataProcess data should:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Use the Python SDK to query existing dataProcess entities"),(0,n.yg)("li",{parentName:"ol"},"Create equivalent dataFlow and dataJob entities"),(0,n.yg)("li",{parentName:"ol"},"Preserve URN mappings for lineage continuity"),(0,n.yg)("li",{parentName:"ol"},"Consider soft-deleting old dataProcess entities once migration is verified")),(0,n.yg)("h3",{id:"graphql-api"},"GraphQL API"),(0,n.yg)("p",null,"The dataProcess entity is minimally exposed in the GraphQL API. Modern GraphQL queries and mutations focus on dataFlow and dataJob entities."),(0,n.yg)("h2",{id:"additional-resources"},"Additional Resources"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"/docs/generated/metamodel/entities/dataflow"},"DataFlow Entity Documentation")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"/docs/generated/metamodel/entities/datajob"},"DataJob Entity Documentation")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"/docs/features/feature-guides/lineage"},"Lineage Documentation")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("a",{parentName:"li",href:"/docs/lineage/airflow"},"Airflow Integration Guide"))),(0,n.yg)("h2",{id:"technical-reference"},"Technical Reference"),(0,n.yg)("p",null,"For technical details about fields, searchability, and relationships, view the ",(0,n.yg)("strong",{parentName:"p"},"Columns")," tab in DataHub."))}m.isMDXComponent=!0}}]);